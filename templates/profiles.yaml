# SLURM Resource Profiles for SPINE Production
# Define common resource configurations for different use cases

profiles:
  ##### S3DF Resource Profiles #####
  # Extremely high-demanding GPU jobs (fastest) at S3DF
  # Node resources: 4 GPUs, 224 CPUs, 1344 GB RAM
  # -> 56 CPU per GPU, 6 GB per CPU
  s3df_hopper:
    site: s3df
    partition: hopper
    gpu_type: h200
    gpu_mem: 141g
    gpus: 1
    cpus_per_task: 56
    mem_per_cpu: 6g
    time: "2:00:00"
    description: "Highest-performance GPU processing on Hopper partition"

  # Large GPU jobs (default) at S3DF
  # This is used to optimize batch sizes for training and inference.
  # Node resources: 4 GPUs, 112 CPUs, 952 GB RAM
  # -> 28 CPU per GPU, 8 GB per CPU
  s3df_ampere:
    site: s3df
    partition: ampere
    gpu_type: a100
    gpu_mem: 40g
    gpus: 1
    cpus_per_task: 28
    mem_per_cpu: 8g
    time: "2:00:00"
    description: "High-performance GPU processing on Ampere partition"

  # Smaller GPU jobs (cheaper) at S3DF
  # Node resources: 10 GPUs, 40 CPUs, 160 GB RAM
  # -> 4 CPUs per GPU, 4 GB per CPU
  s3df_turing:
    site: s3df
    gpu_type: geforce_rtx_2080_ti
    gpu_mem: 11g
    gpus: 1
    cpus_per_task: 4
    mem_per_cpu: 4g
    time: "2:00:00"
    description: "Cheap GPU inference on Turing partition"

  # CPU-only analysis at S3DF Milano
  # Node resources: 120 CPUs, 480 GB RAM
  # -> 4 GB per CPU
  s3df_milano:
    site: s3df
    partition: milano
    gpu_type: null
    gpu_mem: null
    gpus: 0
    cpus_per_task: 1
    mem_per_cpu: 4g
    time: "2:00:00"
    description: "CPU-only analysis tasks"

  # CPU-only analysis at S3DF Roma
  # Node resources: 120 CPUs, 480 GB RAM
  # -> 4 GB per CPU
  s3df_roma:
    site: s3df
    partition: roma
    gpu_type: null
    gpu_mem: null
    gpus: 0
    cpus_per_task: 1
    mem_per_cpu: 4g
    time: "2:00:00"
    description: "CPU-only analysis tasks"

  ##### NERSC Resource Profiles #####
  # NERSC Perlmutter GPU (shared node, 40GB standard variant)
  # 1536 nodes with 4x A100 40GB GPUs (majority of GPU nodes)
  # -> 32 CPUs per GPU (16 physical cores), 128 GB RAM per GPU
  # -> 4 GB per CPU
  # Use this for standard jobs - better availability and queue times
  nersc_gpu:
    site: nersc
    qos: shared
    constraint: gpu
    gpu_type: a100
    gpu_mem: 40g
    gpus_per_node: 1
    cpus_per_task: 16
    mem: 54g
    time: "2:00:00"
    description: "GPU processing on NERSC Perlmutter (shared node, 40GB A100)"

  # NERSC Perlmutter GPU (shared node, 80GB high-memory variant)
  # 256 nodes with 4x A100 80GB GPUs (less available, longer queues)
  # Use only when you specifically need >40GB GPU memory
  nersc_gpu_80gb:
    site: nersc
    qos: shared
    constraint: gpu
    gpu_type: a100
    gpu_mem: 80g
    gpus_per_node: 1
    cpus_per_task: 16
    mem: 54g
    time: "2:00:00"
    description: "High-memory GPU on NERSC Perlmutter (shared node, 80GB A100)"

  # NERSC Perlmutter CPU (shared partition, for small jobs)
  nersc_cpu:
    site: nersc
    qos: shared
    gpu_type: null
    gpu_mem: null
    gpus_per_node: 0
    cpus_per_task: 1
    mem: 4g
    time: "2:00:00"
    description: "CPU-only tasks on NERSC Perlmutter (shared node)"

  # NERSC Perlmutter GPU (full node, exclusive)
  # Use for expensive training requiring full node (40GB standard)
  nersc_gpu_exclusive:
    site: nersc
    qos: regular
    constraint: gpu
    gpu_type: a100
    gpu_mem: 40g
    gpus_per_node: 4
    cpus_per_task: 128
    mem: 0 # Automated full node memory
    time: "2:00:00"
    description: "Full-node GPU processing on NERSC Perlmutter (80GB A100)"

  # NERSC Perlmutter GPU (full node, exclusive)
  # Use for expensive training requiring full node (80GB standard)
  nersc_gpu_exclusive_80gb:
    site: nersc
    qos: regular
    constraint: gpu
    gpu_type: a100
    gpu_mem: 40g
    gpus_per_node: 4
    cpus_per_task: 128
    mem: 0 # Automated full node memory
    time: "2:00:00"
    description: "Full-node GPU processing on NERSC Perlmutter (80GB A100)"

  # NERSC Perlmutter CPU (regular partition)
  # Node resources: 128 CPUs, 512 GB RAM
  # -> 4 GB per CPU
  nersc_cpu_exclusive:
    site: nersc
    qos: regular
    gpu_type: null
    gpu_mem: null
    gpus_per_node: 0
    cpus_per_task: 128
    mem: 0 # Automated full node memory
    time: "2:00:00"
    description: "CPU-only tasks on NERSC Perlmutter"

# Detector-specific defaults
detectors:
  icarus:
    default_profile: s3df_ampere
    configs_dir: infer/icarus
    account: "neutrino:icarus-ml"

  sbnd:
    default_profile: s3df_ampere
    configs_dir: infer/sbnd
    account: "neutrino:icarus-ml"

  2x2:
    default_profile: s3df_ampere
    configs_dir: infer/2x2
    account: "neutrino:dune-ml"

  nd-lar:
    default_profile: s3df_ampere
    configs_dir: infer/nd-lar
    account: "neutrino:dune-ml"

  protodune-vd:
    default_profile: s3df_ampere
    configs_dir: infer/protodune-vd
    account: "neutrino:dune-ml"

  generic:
    default_profile: s3df_ampere
    configs_dir: infer/generic
    account: "neutrino:ml-dev"

  common:
    default_profile: s3df_milano
    configs_dir: infer/common
    account: "neutrino:ml-dev"

# Default settings
defaults:
  account: "neutrino:ml-dev"
  max_array_size: 99
  files_per_task: 1
  output_dir: output
  log_dir: batch_logs
